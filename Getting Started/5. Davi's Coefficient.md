## Davi's Coefficient

By now, you should not only understand the DP definition, but know some DP mechanisms as well. It's awesome that you've learned so much!

But what values of ğœ€ in a ğœ€-differentially private mechanism are good and really preserve the privacy? How to understand the impact of the ğœ€ value?

Thinking about this question, we developed a more intuitive way of understanding such value, and we called it Davi's Coefficient.

Let's remember the Coin Mechanism, presented in the second chapter here. There were two possible coin tosses: the first to define whether or not the answer should be saved as it was spoken. If it was defined that it should be generated by a coin, there was a second coin toss. All tosses, initially, were 50% of chance of outputting heads, just like the image below, where A is the spoken answer:

![](images/coinProbability.png)

And we already learned that such mechanism is ln3-differentially private.

We also know that the ğœ€ value is a measurement of the (lack of) privacy in a DP mechanism. And that the same definition of ğœ€ applies to all DP mechanisms.

The good thing about the Coin Mechanism is its relatively easy understanding. So, we decided to change this experiment in order to get different values for ğœ€. The modification we made was in the first coin toss. Instead of getting a 50% chance of getting heads, we decided we would get a D chance of getting heads. In order words, the probability of saving the answer as it was spoken (and not generating it artificially, without the influence of the spoken answer) will be D.

![](images/daviProbability.png)

**D is called the Davi's Coefficient and can be calculated based on the ğœ€ value we want to achieve. It represents the chance of saving the answer as is originally was, if a Coin Method with the same privacy level was used.**

## Calculating D based on ğœ€

The formula to find the Davi's coefficient associated with a defined ğœ€ is:

D(ğœ€) = [exp(ğœ€)-1]/[exp(ğœ€)+1]

Demonstration:

The DP constraints are:
```
ğ‘ƒ[M(yes) = yes]/ğ‘ƒ[M(no) = yes] â‰¤ exp(ğœ€)
ğ‘ƒ[M(no) = yes]/ğ‘ƒ[M(yes) = yes] â‰¤ exp(ğœ€)
ğ‘ƒ[M(yes) = no]/ğ‘ƒ[M(no) = no] â‰¤ exp(ğœ€)
ğ‘ƒ[M(no) = no]/ğ‘ƒ[M(yes) = no] â‰¤ exp(ğœ€)
```
Adding the probabilities:
```
[D + 0.5*(1-D)]/[0.5*(1-D)] â‰¤ exp(ğœ€)
[0.5*(1-D)]/[D + 0.5*(1-D)] â‰¤ exp(ğœ€)
[0.5*(1-D)]/[D + 0.5*(1-D)] â‰¤ exp(ğœ€)
[D + 0.5*(1-D)]/[0.5*(1-D)] â‰¤ exp(ğœ€)
```
```
(1+D)/(1-D) â‰¤ exp(ğœ€)
(1-D)/(1+D) â‰¤ exp(ğœ€)
(1-D)/(1+D) â‰¤ exp(ğœ€)
(1+D)/(1-D) â‰¤ exp(ğœ€)
```
Using only the most restricting cases, we can simplify writing:
```
(1+D)/(1-D) â‰¤ exp(ğœ€)
```

And, finally, in the worst case:
```
D = [exp(ğœ€)-1]/[exp(ğœ€)+1]
```

