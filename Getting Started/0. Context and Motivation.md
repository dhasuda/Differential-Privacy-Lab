# Introduction

It is noticeable that data is an important asset of the globalized world. Every minute, Google conducts 3.877.140 searches and 4.333.560 YouTube videos are viewed. There is, every moment, a lot of new data being generated by all the users of the internet worldwide, and it is not simply trash data, it is actually useful, and that why many companies invest a lot in storing and processing all the data they can collect.
Amazon.com, for instance, developed its Recommender System using from almost all of its users. In one of Amazon’s first approaches, they searched for users with similar interests, and made suggestions based on this similarity.
Generally, for companies to understand how the user experience is evolving with the product, they have to collect user data. And there are a lot of ways of doing that, and a lot of research about this topic. All of this shows how important data can be and how it can shape the final product delivered to the customer.
Another important factor of the globalized world is the fact that some of this data is used to train neural networks, and very often this training requires a great amount of data. Face ID, for example, the Apple’s system to recognize someone’s face and authenticate based on that, took over 1 billion images to train its neural network.
With all of that in mind, is expected that all collectable data is being collected – or at least there are people trying to collect it. Another example of intense use of data is in Target Corporation Inc, where they wanted to know which customers were pregnant, so they could do focused promotions for them. But, in the end, part of their strategy also surrounds not letting the pregnant customers know that Target knew they were pregnant. It is due to the fact that it would
1
make most of this customer suspicious about how Target got this information, so they would avoid buying at Target. This comes to show that personal data is also sensitive and must not be abused.
The world has come to a point where the focus of getting and processing as much data as possible sometimes blind us about the dangers of making it with no or little responsibility. It begs for the need of updated regulations and processes in order to preserve people’s privacy.

## Motivation
In the midst of the frenzy of collecting data, many times privacy is jeopardized. One of the most emblematic case was the scandal involving Facebook and Cambridge Analytica, where Facebook provided the data and Cambridge Analytica used it improperly to influence the presidential run in the United States.
In this case, Facebook API let developers have access not only to data from people that gave them this permission, but also access their friend’s data on Facebook. This way, Cambridge Analytica had access to data from over 50 million people. And all of this was used to leverage Donald Trump’s campaign.
There are other cases of data breach. Another example was with Tanium, a cybersecurity startup, that exposed the network of a client without permission.
Concerned about these privacy scandals, there are some efforts emerging in order to preserve privacy. GDPR, for instance, is the General Data Protection Regulation from the European Union that rewrites how data sharing must work on the internet. It’s a government effort and establish constraints and rules when accessing user’s data or sharing it. It also establishes some policies for intervention on the stored data.
2
Another effort, and the one that will be the focus of this work, is Differential Privacy. This establishes constraints to algorithms that concentrate data in a statistical database. Such constraints limit the privacy impact on individuals whose data is in the database.
Another factor that is important to keep in mind is that some simple anonymization processes can be very ineffective. Take for example the 2006 Netflix Prize, a competition promoted by Netflix where competitors must develop a algorithm to predict ratings from users. For that, Netflix shared a dataset with over 100 million ratings by over 480 thousand users. All the names were removed, and some fake ratings were added. But as shown later, it was not enough, since a de- anonymization process was possible by comparing the Netflix dataset with an IMDb dataset.
So, the process that privatize data must also be linkage attack-proof. Also, it must not compromise the final result of machine learning algorithms and statistical studies where they are used.
Fortunately, Differential Privacy takes that in count. And also provides a way of measuring privacy. After the privatization process, it is expected that the data still useful. But most of the papers are theory focused, and it is not broadly used yet. It is missing more pragmatic ways of implementing DP algorithms. In Table 1 there is a selection of DP papers and a column distinguishing its focus (whether it has a theorical focus, a practical focus, or there is a balance between both).
